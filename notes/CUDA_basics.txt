docs: https://docs.nvidia.com/cuda/cuda-c-programming-guide/


quick defintions:
    cudaError_t cudaMemcpy(void *dst, const void *src, size_t count, cudaMemcpyKind kind)
        ex: cudaMemcpy(d_in, h_in, size, cudaMemcpyHostToDevice);



reminder:
    memory hierarchy:
        TLDR (fast to slow):
            Registers > Shared > Constant > Global 
        registers: 
            - fastest, meant for local variable calculations
            - 32-64 bytes per thread practical usage (gpu dependant)
            - int, float inside kernel
            - thread private
        shared memory/L1:
            - still pretty fast, used if registers overflow
            - block local, enable cooperation between threads
            - 48-164 kb per SM? (gpu dependant)
        L2 cache:
            - gpu wide
            - 2-40 mb per SM combined (gpu dependant)
            - __shared__ int arr[256];
        global memory:
            - slowest, high latency
            - larger device wide DRAM
            - 4-80 gb (gpu dependant)
            - __device__ or kernel arguments
        constant memory:
            - read only 
            - cached on chip for all threads
            - 64 kb per device
            - __constant__ float coeffs[64];
    terminology:
        TLDR:
            Thread > Warp > Block > Grid
        SM:
            - multiple per gpu
            - hardware unit that executes blocks/warps 
            - contains registers, shared memory, warp schedulers
        thread:
            - smallest unit
            - executes kernel combined
            - has private registers
        warp:
            - group of 32 threads
            - executed SIMD-style on SM
        block:
            - group of threads that shared memory
            - blocks subdivided into warps by gpu hardware for execution
            - synchronized with __syncthreads()
            - allocated on SM
        grid:
            - groups of blocks that make up kernel launch




__global__
    "declares function as being a kernel"
        (kernel: "c++ function meant to be executed in parallel on a GPU)
        - executable on device
        - callable from host
    
variables inside cuda functions:
    cuda runtiume injects a couple variables into every kernel meant for coordinates
    when kernel launched, cuda creates grid dimension * block dimension threads
        (from [function name]<<<gridDim, blockDim>>>())
    thread identifiers:
        threadIdx.x
        threadIdx.y
        threadIdx.z
    block dimensions:
        blockDim.x
        blockDim.y
        blockDim.z
    block identifiers:
        blockIdx.x
        blockIdx.y
        blockIdx.z
    grid dimensions:
        gridDim.x
        gridDim.y
        gridDim.z
    (?)special per-thread registers:cudaError_t cudaMemcpy(void *dst, const void *src, size_t count, cudaMemcpyKind kind)
        warpSize
        __lane_id()
        __activemask()
        __syncthreads_count()

__device__

vec_add and vec_add_kernel:
    methodology: must create on host, send to gpu memory for kernel, send back
        this is necessary since gpu cannot access host memory
    remember that the SM designates a copy of the kernel per thread and each kernel
        with its designated x,y,z block/thread runs only once per block
    therefore, we should always make sure there are ENOUGH THREADS TO COVER ALL ITEMS
        note: in the case, vec_add_kernel has if-statement that will simply not run the
            function if the designated block, thread id is greater than the number of 
            items
    when to use multiple blocks?
        possible for there to be more items to handle than threads available
        typically limited to 1024 threads per block on modern groups
        ex + how to guard:
            int n = 5000;                // vector length
            int threads_per_block = 256; // common choice
            int blocks = (n + threads_per_block - 1) / threads_per_block; // 20 blocks
            ...
            [function]<<<blocks, threads_per_block>>>();


shared_mem and shared_mem_kernel:
    extern:
        size of memory used defined dynamically at launch
    __shared__:
        variable (in this case an array) is on shared memory:
            - fast on chip memory
            - all blocks can see and utilize
            - scoped per block, so each block has its own shared memory instance
        NOTE:
            third value in [function]<<<block, thread, shared_memory_in_bytes_per_block>>>()
    __syncthreads:
        in this case, must make sure all threads are done writing to tile array before 
            reading from it into out array. this is to prevent race conditions


warp_reduce, warp_reduce_kernel and warp_reduce_sum:
    methodology:
        1. each thread gets a unique value designated in warp_reduce_KERNEL
        2. warp_reduce_SUM is stored on the device
            __inline__:
                code inserted at call site ... this might be outdated? modern usages have heuristics?
            __device__:
                code designated to be sent to GPU memory
                cannot be called except by __global__ function
        3. each thread executes warp_reduce_SUM:
            3.1: values <offset> distance away are read, then summed to current
            3.2: at each step, offset is halved and the values sum this new value into current location
                NOTE: all threads execute this the same number of times as offset changes
                ...so thread 31 executes the same number of times as thread 1 BUT since the offset will
                    look for 31 + offset, it will be out of bounds (autohandled or return same value)
                AND THIS IS FINE since at the end we only care about the value at index 0 on thread 0
                TODO?: i wonder if we could free the unused threads by applying some binary calculations
                    on the masks at each offset step?
            3.3: loop exits and the value returns
            3.4: we check for the (thread x & 31) == 0 which gives thread 0 with the final result and
                only then do we set that as the out result
                NOTE: this is setup for multiple warp runs, since we check every thread.x that is divisible
                    by 31 aka threads at every warp step