{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grahamstelzer/fundamentals/blob/main/transformer_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1g4GKHUv4YM"
      },
      "outputs": [],
      "source": [
        "# setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYAIp0JTwuy6",
        "outputId": "62674ba0-ca80-4e81-845c-d893e519b26b"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "print(\"pytorch version: \", torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaXSqFfuwzpz"
      },
      "outputs": [],
      "source": [
        "# \"config\"\n",
        "batch_size = 2\n",
        "seq_len = 4\n",
        "input_dim = 16\n",
        "d_model = 8\n",
        "num_heads = 2\n",
        "d_ff = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6sRrK5exXjL"
      },
      "outputs": [],
      "source": [
        "# \"base\" mha\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    # constructor\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__() # call nn.Module constructor\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        # member variables\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        # set transformations for weights\n",
        "        #   q, k, v and output\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # forward\n",
        "    def forward(self, x):\n",
        "\n",
        "        # shape usually (batch_size, seq_length, d_model) but sometimes not\n",
        "        B, S, _ = x.shape\n",
        "\n",
        "        # apply transformations\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # change tensors from (B, S, d_model) -> (B, S, head_idx, d_model // num_heads)\n",
        "        #   or in other words, split the d_model dimension into 1 section per head\n",
        "        #   (this is why we need to do assert d_model % num heads == 0, otherwise uneven split)\n",
        "\n",
        "        def reshape(t):\n",
        "            t_reshaped = t.view(B, S, self.num_heads, self.head_dim)\n",
        "            t_heads = t_reshaped.transpose(1, 2)\n",
        "            return t_heads\n",
        "\n",
        "        Qh = reshape(Q)\n",
        "        Kh = reshape(K)\n",
        "        Vh = reshape(V)\n",
        "\n",
        "\n",
        "        # scaled dot product attention:\n",
        "        scores = Qh @ Kh.transpose(-2, -1) # Q * K_t\n",
        "        scores = scores / math.sqrt(self.head_dim) # divide by sqrt d_model accounting for multihead\n",
        "\n",
        "        attn = scores.softmax(dim=-1) # softmax\n",
        "\n",
        "        out = attn @ Vh # multiply by V\n",
        "\n",
        "        out = out.transpose(1,2).contiguous().view(B, S, self.d_model) # combine heads\n",
        "\n",
        "        out = self.W_o(out) # final linear transform\n",
        "\n",
        "        return out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50DuWNPuzebj"
      },
      "outputs": [],
      "source": [
        "# feed forward (position-wise)\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqUFgje8Q9XF"
      },
      "outputs": [],
      "source": [
        "# \"transformer block\" - though i think this could also be referred to as \"encoderblock\"\n",
        "#   apparently standard convention these days is to use a transformer block with toggleable\n",
        "#   cross attention and attention mask, then use for both \"encoder\" and \"decoder\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FFN(d_model, d_ff)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # mha and add/norm\n",
        "        x_norm = self.norm1(x)\n",
        "        attn_out = self.mha(x_norm)\n",
        "        x = x + attn_out\n",
        "\n",
        "        # ff and add/norm again\n",
        "        x_norm2 = self.norm2(x)\n",
        "        ffn_out = self.ffn(x_norm2)\n",
        "        x = x + ffn_out\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWWo56bffND_"
      },
      "outputs": [],
      "source": [
        "# \"encoder\" (not encoderblock), will stack the tformerblock\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, num_heads, d_ff, num_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # must make sure input is correct dimension (? double check)\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        # setup block layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, d_ff)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAFkee4yhWzc",
        "outputId": "b130ae0d-c7ed-493f-be1d-56a0aafcfcc4"
      },
      "outputs": [],
      "source": [
        "# \"main\" to test just the model code\n",
        "x = torch.randn(batch_size, seq_len, input_dim)\n",
        "encoder = Encoder(input_dim, d_model, num_heads, d_ff, num_layers=2)\n",
        "\n",
        "output = encoder(x)\n",
        "print(output.shape)\n",
        "print(output)\n",
        "print(output.grad)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPzGCYcmVw4mpHxqptrsAG9",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
