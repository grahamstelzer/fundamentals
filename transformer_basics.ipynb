{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzGCYcmVw4mpHxqptrsAG9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grahamstelzer/fundamentals/blob/main/transformer_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "-1g4GKHUv4YM"
      },
      "outputs": [],
      "source": [
        "# setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "print(\"pytorch version: \", torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYAIp0JTwuy6",
        "outputId": "62674ba0-ca80-4e81-845c-d893e519b26b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch version:  2.9.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"config\"\n",
        "batch_size = 2\n",
        "seq_len = 4\n",
        "input_dim = 16\n",
        "d_model = 8\n",
        "num_heads = 2\n",
        "d_ff = 32"
      ],
      "metadata": {
        "id": "GaXSqFfuwzpz"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"base\" mha\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    # constructor\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__() # call nn.Module constructor\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        # member variables\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        # set transformations for weights\n",
        "        #   q, k, v and output\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # forward\n",
        "    def forward(self, x):\n",
        "\n",
        "        # shape usually (batch_size, seq_length, d_model) but sometimes not\n",
        "        B, S, _ = x.shape\n",
        "\n",
        "        # apply transformations\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # change tensors from (B, S, d_model) -> (B, S, head_idx, d_model // num_heads)\n",
        "        #   or in other words, split the d_model dimension into 1 section per head\n",
        "        #   (this is why we need to do assert d_model % num heads == 0, otherwise uneven split)\n",
        "\n",
        "        def reshape(t):\n",
        "            t_reshaped = t.view(B, S, self.num_heads, self.head_dim)\n",
        "            t_heads = t_reshaped.transpose(1, 2)\n",
        "            return t_heads\n",
        "\n",
        "        Qh = reshape(Q)\n",
        "        Kh = reshape(K)\n",
        "        Vh = reshape(V)\n",
        "\n",
        "\n",
        "        # scaled dot product attention:\n",
        "        scores = Qh @ Kh.transpose(-2, -1) # Q * K_t\n",
        "        scores = scores / math.sqrt(self.head_dim) # divide by sqrt d_model accounting for multihead\n",
        "\n",
        "        attn = scores.softmax(dim=-1) # softmax\n",
        "\n",
        "        out = attn @ Vh # multiply by V\n",
        "\n",
        "        out = out.transpose(1,2).contiguous().view(B, S, self.d_model) # combine heads\n",
        "\n",
        "        out = self.W_o(out) # final linear transform\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "t6sRrK5exXjL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feed forward (position-wise)\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "50DuWNPuzebj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"transformer block\" - though i think this could also be referred to as \"encoderblock\"\n",
        "#   apparently standard convention these days is to use a transformer block with toggleable\n",
        "#   cross attention and attention mask, then use for both \"encoder\" and \"decoder\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FFN(d_model, d_ff)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # mha and add/norm\n",
        "        x_norm = self.norm1(x)\n",
        "        attn_out = self.mha(x_norm)\n",
        "        x = x + attn_out\n",
        "\n",
        "        # ff and add/norm again\n",
        "        x_norm2 = self.norm2(x)\n",
        "        ffn_out = self.ffn(x_norm2)\n",
        "        x = x + ffn_out\n",
        "\n",
        "        return x\n",
        ""
      ],
      "metadata": {
        "id": "oqUFgje8Q9XF"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"encoder\" (not encoderblock), will stack the tformerblock\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, num_heads, d_ff, num_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # must make sure input is correct dimension (? double check)\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        # setup block layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, d_ff)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "HWWo56bffND_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"main\" to test just the model code\n",
        "x = torch.randn(batch_size, seq_len, input_dim)\n",
        "encoder = Encoder(input_dim, d_model, num_heads, d_ff, num_layers=2)\n",
        "\n",
        "output = encoder(x)\n",
        "print(output.shape)\n",
        "print(output)\n",
        "print(output.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAFkee4yhWzc",
        "outputId": "b130ae0d-c7ed-493f-be1d-56a0aafcfcc4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 8])\n",
            "tensor([[[ 0.6514, -1.1451, -0.6806, -1.2014,  0.1812,  1.6664, -2.6184,\n",
            "           0.7552],\n",
            "         [-0.6710, -1.4770,  0.1584, -0.7289, -0.1058,  1.2851, -0.3658,\n",
            "           0.8349],\n",
            "         [-0.5325, -0.3263, -0.6118, -0.2680,  1.5829,  0.2944, -1.1985,\n",
            "          -0.7723],\n",
            "         [-0.1372, -0.8505,  0.3939, -0.6812,  0.5379,  0.4848, -0.5404,\n",
            "           0.0939]],\n",
            "\n",
            "        [[-0.1782, -0.9927, -0.4383, -0.3518,  0.3448,  1.1385, -0.3957,\n",
            "           0.7862],\n",
            "         [-0.1746, -1.0193,  0.3260, -0.8109,  0.9293,  0.1760, -0.4882,\n",
            "          -0.4234],\n",
            "         [ 0.4474, -1.5515, -0.4870, -0.6148,  0.8368,  0.2902, -0.2324,\n",
            "           0.1455],\n",
            "         [ 0.0474, -0.3549,  0.2178, -0.2285,  0.8008,  0.5741, -1.9748,\n",
            "           0.2921]]], grad_fn=<AddBackward0>)\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-945291916.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print(output.grad)\n"
          ]
        }
      ]
    }
  ]
}